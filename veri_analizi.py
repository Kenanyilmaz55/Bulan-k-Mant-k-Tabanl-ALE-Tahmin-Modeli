# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sf5S7dWLLGSXGSjYoQy_gJb9bafyW97d
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def load_and_analyze_csv(file_path='veri.csv'):
    """
    CSV dosyasını yükle ve detaylı analiz yap
    """
    try:
        # CSV dosyasını yükle
        print("CSV dosyası yükleniyor...")
        data = pd.read_csv(file_path)

        print(f"✓ Veri başarıyla yüklendi!")
        print(f"  - Boyut: {data.shape[0]} satır × {data.shape[1]} sütun")

        # Sütun isimlerini kontrol et ve standartlaştır
        print(f"\nSütun isimleri:")
        for i, col in enumerate(data.columns):
            print(f"  {i+1}. {col}")

        # Eğer sütun isimleri farklıysa standart isimleri ata
        expected_columns = ['anchor_ratio', 'transmission_range', 'node_density', 'iteration_count', 'ALE', 'std_dev']

        if len(data.columns) == 6:
            data.columns = expected_columns
            print(f"\n✓ Sütun isimleri standartlaştırıldı")
        elif len(data.columns) == 5:
            # Standart sapma sütunu yoksa
            data.columns = expected_columns[:-1]
            print(f"\n✓ Sütun isimleri standartlaştırıldı (standart sapma sütunu yok)")

        # Veri tiplerini kontrol et
        print(f"\nVeri tipleri:")
        print(data.dtypes)

        # Temel istatistikler
        print(f"\nTemel İstatistikler:")
        print(data.describe())

        # Eksik değer kontrolü
        missing_values = data.isnull().sum()
        if missing_values.any():
            print(f"\n⚠️  Eksik değerler tespit edildi:")
            for col, count in missing_values.items():
                if count > 0:
                    print(f"  - {col}: {count} eksik değer")

            # Eksik değerleri doldur
            print(f"\n📊 Eksik değerler ortalama ile doldurulacak...")
            data = data.fillna(data.mean())
            print(f"✓ Eksik değerler dolduruldu")
        else:
            print(f"\n✓ Eksik değer bulunmuyor")

        # Aykırı değer kontrolü
        print(f"\n🔍 Aykırı değer kontrolü:")
        for col in data.select_dtypes(include=[np.number]).columns:
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]
            if len(outliers) > 0:
                print(f"  - {col}: {len(outliers)} aykırı değer tespit edildi")
            else:
                print(f"  - {col}: Aykırı değer yok")

        # Korelasyon analizi
        print(f"\n📈 Korelasyon Analizi:")
        correlation_matrix = data.corr()
        print(correlation_matrix.round(3))

        # Görselleştirme
        create_data_visualizations(data)

        return data.values, data

    except FileNotFoundError:
        print(f"❌ Hata: '{file_path}' dosyası bulunamadı!")
        print("Lütfen dosya yolunu kontrol edin veya dosyayı doğru konuma yerleştirin.")
        return None, None

    except Exception as e:
        print(f"❌ Veri yükleme hatası: {str(e)}")
        return None, None

def create_data_visualizations(data):
    """
    Veri seti için görselleştirmeler oluştur
    """
    print(f"\n🎨 Görselleştirmeler oluşturuluyor...")

    # Figure boyutunu ayarla
    plt.style.use('default')

    # 1. Veri dağılımları
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('WSN Veri Seti - Özellik Dağılımları', fontsize=16, fontweight='bold')

    feature_names = ['Çapa Oranı', 'İletim Aralığı', 'Düğüm Yoğunluğu', 'Yineleme Sayısı', 'ALE']

    for i, (col, name) in enumerate(zip(data.columns[:5], feature_names)):
        row = i // 3
        col_idx = i % 3

        axes[row, col_idx].hist(data[col], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        axes[row, col_idx].set_title(f'{name} Dağılımı')
        axes[row, col_idx].set_xlabel(name)
        axes[row, col_idx].set_ylabel('Frekans')
        axes[row, col_idx].grid(True, alpha=0.3)

        # İstatistikleri ekle
        mean_val = data[col].mean()
        std_val = data[col].std()
        axes[row, col_idx].axvline(mean_val, color='red', linestyle='--',
                                  label=f'Ortalama: {mean_val:.3f}')
        axes[row, col_idx].legend()

    # Son subplot'u gizle
    axes[1, 2].set_visible(False)

    plt.tight_layout()
    plt.show()

    # 2. Korelasyon matrisi heatmap
    plt.figure(figsize=(10, 8))
    correlation_matrix = data.corr()

    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm',
                center=0, square=True, fmt='.3f', cbar_kws={"shrink": .8})
    plt.title('Özellikler Arası Korelasyon Matrisi', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

    # 3. ALE ile diğer özellikler arasındaki ilişki
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ALE ile Giriş Özellikleri Arasındaki İlişki', fontsize=16, fontweight='bold')

    input_features = data.columns[:4]
    feature_names = ['Çapa Oranı', 'İletim Aralığı', 'Düğüm Yoğunluğu', 'Yineleme Sayısı']

    for i, (feature, name) in enumerate(zip(input_features, feature_names)):
        row = i // 2
        col = i % 2

        axes[row, col].scatter(data[feature], data['ALE'], alpha=0.6, color='blue')
        axes[row, col].set_xlabel(name)
        axes[row, col].set_ylabel('ALE')
        axes[row, col].set_title(f'{name} vs ALE')
        axes[row, col].grid(True, alpha=0.3)

        # Trend çizgisi ekle
        z = np.polyfit(data[feature], data['ALE'], 1)
        p = np.poly1d(z)
        axes[row, col].plot(data[feature], p(data[feature]), "r--", alpha=0.8)

        # Korelasyon katsayısını göster
        corr_coef = data[feature].corr(data['ALE'])
        axes[row, col].text(0.05, 0.95, f'r = {corr_coef:.3f}',
                           transform=axes[row, col].transAxes,
                           bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

    plt.tight_layout()
    plt.show()

    print("✓ Görselleştirmeler tamamlandı")

def validate_data_format(data):
    """
    Veri formatını doğrula
    """
    print(f"\n🔍 Veri formatı doğrulanıyor...")

    if data is None:
        print("❌ Veri yüklenemedi!")
        return False

    # Beklenen format: 107 satır, 5 veya 6 sütun
    expected_rows = 107
    min_cols = 5
    max_cols = 6

    actual_rows, actual_cols = data.shape

    print(f"  - Beklenen satır sayısı: {expected_rows}")
    print(f"  - Gerçek satır sayısı: {actual_rows}")
    print(f"  - Beklenen sütun sayısı: {min_cols}-{max_cols}")
    print(f"  - Gerçek sütun sayısı: {actual_cols}")

    if actual_rows != expected_rows:
        print(f"⚠️  Uyarı: Satır sayısı beklenenden farklı!")

    if actual_cols < min_cols or actual_cols > max_cols:
        print(f"❌ Hata: Sütun sayısı beklenen aralıkta değil!")
        return False

    # Veri tiplerini kontrol et
    numeric_data = data.select_dtypes(include=[np.number])
    if len(numeric_data.columns) != len(data.columns):
        print(f"⚠️  Uyarı: Tüm sütunlar sayısal değil!")

    print(f"✓ Veri formatı doğrulandı")
    return True

def prepare_data_for_fuzzy_system(data_array):
    """
    Bulanık sistem için veriyi hazırla
    """
    print(f"\n🔧 Veri bulanık sistem için hazırlanıyor...")

    # Son sütun standart sapma ise çıkar
    if data_array.shape[1] == 6:
        # İlk 4 özellik + ALE (standart sapma hariç)
        processed_data = data_array[:, :5]
        print(f"✓ Standart sapma sütunu çıkarıldı")
    else:
        processed_data = data_array

    print(f"✓ Hazırlanan veri boyutu: {processed_data.shape}")
    print(f"  - Giriş özellikleri: {processed_data.shape[1]-1}")
    print(f"  - Çıkış değişkeni: 1 (ALE)")

    return processed_data

# Test fonksiyonu
if __name__ == "__main__":
    print("=" * 60)
    print("WSN VERİ SETİ YÜKLEME VE ANALİZ MODÜLÜ")
    print("=" * 60)

    # Veri yükleme ve analizi
    data_array, data_df = load_and_analyze_csv('veri.csv')

    if data_array is not None:
        # Veri formatını doğrula
        valid = validate_data_format(data_df)

        if valid:
            # Bulanık sistem için hazırla
            processed_data = prepare_data_for_fuzzy_system(data_array)
            print(f"\n✅ Veri başarıyla yüklendi ve işlendi!")
            print(f"   Bulanık sisteme hazır veri boyutu: {processed_data.shape}")
        else:
            print(f"\n❌ Veri formatı uygun değil!")
    else:
        print(f"\n❌ Veri yüklenemedi!")